{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Recognition with LSTMS\n",
    "\n",
    "Hello everybody!\n",
    "This is our *notebook* for Human Activity Recognition with LSTMs with help of simple smartphone-sensor data. \n",
    ">**This is part of an Android-Application-Project which will be used for non-intrusive medical surveillance systems.** \n",
    "\n",
    "\n",
    "### Ressources\n",
    "\n",
    "*Speaking for me,* I'm no expert in HAR with deep-learning.\n",
    "That's why I did some research over already existing projects and tutorials (there are quite some.)\n",
    "Check them out in our **spreasheet under the RESSOURCE-TAB: **\n",
    "https://docs.google.com/spreadsheets/d/1EDc84oX6Z9HOHBKIYNhE6iemN4Il1MR1ylGJyiCkh-M/edit#gid=251000036\n",
    "\n",
    "### What's next?\n",
    "\n",
    "As I said in our *slack-channel* there will be a small team responsible for different parts of this projects.<br > \n",
    "**The topics/groups are the following** *(If you have a better idea, feel free to tell me!)* <br>\n",
    "**--> ADD YOUR NAME BEHIND ONE (OR MORE) OF THE TOPICS AND START WORKING WITH THE OTHERS IN YOUR TEA!!**\n",
    "\n",
    "* _Data-Extraction/Generation and Pre-Processing:_ Standardize, Normalize, Batching-fct., etc.\n",
    "<br> `In Charge: @Nicolas Remerscheid`\n",
    "* _The Model:_ Maybe Embedded Layer (Dim. Reduction), LSTMs, Linear-Layer, etc.\n",
    "<br> `In Charge: ...`\n",
    "* _The Training:_ Training loops, visualization\n",
    "<br> `In Charge: ...`\n",
    "* _Validation and Testing:_ finding best hyperparams (Validation), test-loops, check-points\n",
    "<br> `In Charge: ...`\n",
    "\n",
    "One everybody has chosen a section, feel free to edit this notebook and **add your section!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import os\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data-Extraction/Generation and Pre-Processing\n",
    "For now use mostly the concepts, principles of: \n",
    "> Guillaume Chevalier, LSTMs for Human Activity Recognition, 2016, https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## DEFINED PARAMS ########\n",
    "# DATA: \n",
    "#    - UCI-Dataset: 60999314 data-examples\n",
    "# Model: \n",
    "#    - Input-dimension/features: \n",
    "#    - Output-dimension: 6\n",
    "#    - Seq.-Length: 128 (2.56 sec with 1/2 Hz sensor-output on data in UCL data-set) -> TO BE VERIFIED!\n",
    "#    - Overlap: 50% between windows \n",
    "#    - Batch-Size: 1500 \n",
    "#    -> More Params to be defined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Constants\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "]\n",
    "\n",
    "# More from UCI Data-set and HAR-LSTM-Project from Guillaume Chevalier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
